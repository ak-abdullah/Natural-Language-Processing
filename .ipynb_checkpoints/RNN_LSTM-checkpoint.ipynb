{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf2bd55-f995-4014-b814-d1186b7e9979",
   "metadata": {},
   "source": [
    "English Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7940a28-af57-4d22-ba9b-f6004f71d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import contractions\n",
    "import emoji\n",
    "\n",
    "\n",
    "#functions\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        text = text.lower()  # Lowercase\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "        text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "        text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "        return text\n",
    "    return ''  # Return an empty string for non-string inputs\n",
    "\n",
    "def tokenize_english(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(tokenized_list):\n",
    "    return [word for word in tokenized_list if word not in stop_words]\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def stem_words(words):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "#Reading File\n",
    "file = pd.read_excel('parallel-corpus.xlsx', header = None,usecols=[0, 1], skiprows=1)\n",
    "\n",
    "#Text Cleaning Punctuation etc\n",
    "\n",
    "file[0] = file[0].apply(clean_text)  # Clean English text \n",
    "\n",
    "#contractions\n",
    "\n",
    "file[0] = [''.join(doc) for doc in file[0]]\n",
    "file[0] = file[0].apply(contractions.fix)\n",
    "\n",
    "# handling emojis\n",
    "file[0] = file[0].apply(emoji.demojize)\n",
    "\n",
    "\n",
    "#Saving File\n",
    "file.iloc[:, 0].to_frame(name='English').to_excel('English_Sentences.xlsx', index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3a2d1-bb2d-4a0e-ad49-367099959dc5",
   "metadata": {},
   "source": [
    "Urdu Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc0016-d428-4049-af2a-d149f4dd0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from LughaatNLP import NER_Urdu\n",
    "from LughaatNLP import POS_urdu\n",
    "from LughaatNLP import LughaatNLP\n",
    "\n",
    "\n",
    "urdu_text_processing = LughaatNLP()\n",
    "\n",
    "\n",
    "file = pd.read_excel('parallel-corpus.xlsx', header = None,usecols=[0, 1], skiprows=1)\n",
    "duplicates = file[file.duplicated(keep=False)]\n",
    "\n",
    "file[1] = file[1].apply(lambda x: urdu_text_processing.normalize(str(x)))\n",
    "file[1] = file[1].apply(lambda x: urdu_text_processing.remove_english(str(x)))\n",
    "file[1] = file[1].apply(lambda x: urdu_text_processing.remove_urls(str(x)))\n",
    "file[1] = file[1].apply(lambda x: urdu_text_processing.remove_special_characters(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_excel('English_Sentences.xlsx')\n",
    "\n",
    "df['Urdu'] = file.iloc[:, 1]\n",
    "\n",
    "# Save the updated DataFrame back to the same file with both columns\n",
    "df.to_excel('English_Urdu.xlsx', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0ada1-2995-45b0-8983-eea8da2e39c6",
   "metadata": {},
   "source": [
    "RNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b3390-28dc-4102-be8a-6466a6e6f3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel('English_Urdu.xlsx')\n",
    "df = df.astype(str)\n",
    "# Split the data into input (English) and output (Urdu)\n",
    "english_sentences = df['English'].tolist()\n",
    "urdu_sentences = df['Urdu'].tolist()\n",
    "\n",
    "# Create tokenizers for English and Urdu\n",
    "english_tokenizer = Tokenizer()\n",
    "urdu_tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizers to the data\n",
    "english_tokenizer.fit_on_texts(english_sentences)\n",
    "urdu_tokenizer.fit_on_texts(urdu_sentences)\n",
    "\n",
    "# Convert text to sequences\n",
    "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
    "urdu_sequences = urdu_tokenizer.texts_to_sequences(urdu_sentences)\n",
    "\n",
    "# Pad the sequences\n",
    "max_english_length = 40\n",
    "max_urdu_length = 40\n",
    "padded_english = pad_sequences(english_sequences, maxlen=max_english_length, padding='post')\n",
    "padded_urdu = pad_sequences(urdu_sequences, maxlen=max_urdu_length, padding='post')\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_size = int(0.8 * len(padded_english))\n",
    "val_size = int(0.1 * len(padded_english))\n",
    "\n",
    "train_english = padded_english[:train_size]\n",
    "train_urdu = padded_urdu[:train_size]\n",
    "val_english = padded_english[train_size:train_size+val_size]\n",
    "val_urdu = padded_urdu[train_size:train_size+val_size]\n",
    "test_english = padded_english[train_size+val_size:]\n",
    "test_urdu = padded_urdu[train_size+val_size:]\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, TimeDistributed, Bidirectional\n",
    "\n",
    "# Define the RNN-based model\n",
    "\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=len(english_tokenizer.word_index)+1, output_dim=128),\n",
    "    Bidirectional(SimpleRNN(128, return_sequences=True, activation='tanh')),\n",
    "    TimeDistributed(Dense(len(urdu_tokenizer.word_index)+1, activation='softmax'))\n",
    "])\n",
    "# Compile the model\n",
    "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint('english_to_urdu_rnn_model.keras', \n",
    "                             monitor='val_accuracy', \n",
    "                             mode='max', \n",
    "                             save_best_only=True, \n",
    "                             verbose=1)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "rnn_model.fit(train_english, train_urdu, epochs=25, batch_size=64, \n",
    "              validation_data=(val_english, val_urdu), \n",
    "              callbacks=[checkpoint], \n",
    "              verbose=1)\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "def calculate_bleu(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for machine translation model.\n",
    "\n",
    "    Args:\n",
    "    y_true (numpy array): True sentences.\n",
    "    y_pred (numpy array): Predicted sentences.\n",
    "\n",
    "    Returns:\n",
    "    float: Mean BLEU score.\n",
    "    \"\"\"\n",
    "    smoothing_func = SmoothingFunction()\n",
    "    bleu_scores = []\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        # Ignore padding tokens and handle unknown words\n",
    "        true_sentence = []\n",
    "        for word in y_true[i]:\n",
    "            if word != 0:  # ignore padding tokens\n",
    "                word = urdu_tokenizer.index_word.get(word, '')\n",
    "                if word:  # ignore empty strings (unknown words)\n",
    "                    true_sentence.append(word)\n",
    "\n",
    "        # Get indices of top words in y_pred\n",
    "        pred_indices = np.argmax(y_pred[i], axis=1)\n",
    "        \n",
    "        # Get words from indices\n",
    "        pred_sentence = [urdu_tokenizer.index_word.get(word, '') for word in pred_indices]\n",
    "        pred_sentence = [word for word in pred_sentence if word]  # ignore unknown words\n",
    "\n",
    "        # Tokenize sentences\n",
    "        true_sentence = word_tokenize(' '.join(true_sentence))\n",
    "        pred_sentence = word_tokenize(' '.join(pred_sentence))\n",
    "\n",
    "        # Calculate BLEU score with smoothing\n",
    "        score = sentence_bleu([true_sentence], pred_sentence, smoothing_function=smoothing_func.method4)\n",
    "        bleu_scores.append(score)\n",
    "\n",
    "    return np.mean(bleu_scores)# Evaluate the model\n",
    "test_pred = rnn_model.predict(test_english)\n",
    "test_bleu = calculate_bleu(test_urdu, test_pred)\n",
    "print(f'Test BLEU score: {test_bleu:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def translate(sentence):\n",
    "    sequence = english_tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_english_length, padding='post')\n",
    "    prediction = rnn_model.predict(padded_sequence)\n",
    "    translation = []\n",
    "    for word_prob in prediction[0]:\n",
    "        predicted_index = np.argmax(word_prob)\n",
    "        word = urdu_tokenizer.index_word.get(predicted_index, 'UNK')\n",
    "        translation.append(word)\n",
    "    return ' '.join(translation)\n",
    "\n",
    "\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"I am from Pakistan.\",\n",
    "    \"How old are you?\",\n",
    "    \"What do you do?\",\n",
    "    \"I love reading books.\",\n",
    "    \"Where do you live?\",\n",
    "    \"What is your favorite food?\",\n",
    "    \"I am learning Urdu.\",\n",
    "    \"Goodbye, take care.\"\n",
    "]\n",
    "\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    predicted_translation = translate(sentence)\n",
    "    print(f\"Input: {sentence}\")\n",
    "    print(f\"Predicted Translation: {predicted_translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360b7b2-469c-4966-bf92-3c224b0b99a6",
   "metadata": {},
   "source": [
    "RNN Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee2502e-75c7-403c-ba62-2d8ec640401b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Input: how can i communicate with my parents\n",
      "Predicted Translation: میں اپنے والدین سے کیسے بات کہا UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: میں اپنے والدین سے کیسے بات کروں\n",
      "BLEU Score: 0.1132\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Input: how can i make friends \n",
      "Predicted Translation: میں اس کیسے بنائوں سکتا UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: میں دوست کیسے بنائوں\n",
      "BLEU Score: 0.0175\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Input: why do i get so sad \n",
      "Predicted Translation: میں اتنا اداس کیوں UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: میں اتنا اداس کیوں ہوں\n",
      "BLEU Score: 0.0575\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Input: if you ve asked yourself such questions you re not alone\n",
      "Predicted Translation: اگر اپ نے کو اپ ہوئے کیا سوالات کیے گے وہ گے سکتے سکتے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: اگر اپ نے اپنے اپ سے ایسے سوالات کیے ہیں تو اپ اکیلے نہیں ہیں\n",
      "BLEU Score: 0.0417\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Input: depending on where you ve turned for guidance you may have been given conflicting answers\n",
      "Predicted Translation: اس بات پر منحصر ہے اگر اپ رہنمائی کے لیے کرتے ہیں ہیں ہو سکتا اپ اپ نے متضاد کر ہو ہیں UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: اس بات پر منحصر ہے کہ اپ رہنمائی کے لیے کہاں گئے ہیں ہو سکتا ہے اپ کو متضاد جوابات دیے گئے ہوں\n",
      "BLEU Score: 0.1793\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Input: to help young people get solid advice they can rely on awake magazine launched the biblebased series entitled  young people ask \n",
      "Predicted Translation: نوجوانوں سے ٹھوس مشورے حاصل کرنے میں مدد کرنے کے لیے جس پر وہ بھروسہ کر سکتے ہیں جاگو میگزین نے بائبل کے کے رسالہ کر کیا UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: نوجوانوں کو ٹھوس مشورے حاصل کرنے میں مدد کرنے کے لیے جس پر وہ بھروسہ کر سکتے ہیں جاگو میگزین نے بائبل پر مبنی رسالہ شروع کیا\n",
      "BLEU Score: 0.4969\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "Input: in january decades later the series still draws an enthusiastic response\n",
      "Predicted Translation: ۸ جنوری ۱۹۸۲ دہائیوں کے بعد سیریز اب بھی ایک پرجوش ردعمل کھینچتی ہے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: ۸ جنوری ۱۹۸۲ دہائیوں کے بعد سیریز اب بھی ایک پرجوش ردعمل کھینچتی ہے\n",
      "BLEU Score: 0.3235\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Input: each article is the product of extensive research in fact to determine just how young people think and feel awake\n",
      "Predicted Translation: درحقیقت اس بات کا تعین کرنے کے لیے کہ نوجوان حساب سوچتے UNK جانی کرتے UNK جاگو UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: درحقیقت اس بات کا تعین کرنے کے لیے کہ نوجوان کیسے سوچتے اور محسوس کرتے ہیں جاگو\n",
      "BLEU Score: 0.2338\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Input: the book you now hold was originally published in \n",
      "Predicted Translation: تم کتاب اپ کو کرو کچھ وہ اصل میں ۱۹۸۹ میں ہوئی تھی UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: جو کتاب اپ کے پاس ہے وہ اصل میں ۱۹۸۹ میں شائع ہوئی تھی\n",
      "BLEU Score: 0.1102\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Input: however the chapters have been completely revised to address the issues of today\n",
      "Predicted Translation: کراچی کے مسائل کو حل کرنے کے لیے ابواب پر لیے مزید ثانی کی گئی UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: اج کے مسائل کو حل کرنے کے لیے ابواب پر مکمل نظر ثانی کی گئی ہے\n",
      "BLEU Score: 0.2264\n",
      "\n",
      "Overall BLEU Score: 0.1800\n",
      "BLEU Scores for each sentence:\n",
      "Sentence 1: 0.1132\n",
      "Sentence 2: 0.0175\n",
      "Sentence 3: 0.0575\n",
      "Sentence 4: 0.0417\n",
      "Sentence 5: 0.1793\n",
      "Sentence 6: 0.4969\n",
      "Sentence 7: 0.3235\n",
      "Sentence 8: 0.2338\n",
      "Sentence 9: 0.1102\n",
      "Sentence 10: 0.2264\n",
      "\n",
      "Additional Test Sentences:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "Input: How are you today?\n",
      "Predicted Translation: اج اج کتنے ہیں ہو UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "Input: What is your favorite food?\n",
      "Predicted Translation: اپ کے کیا سے ہے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Input: I love reading books.\n",
      "Predicted Translation: میں زندگی پڑھ کتابیں پسند ہے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Input: Where do you live?\n",
      "Predicted Translation: اپ کہاں رہتے ہیں UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Input: What do you do?\n",
      "Predicted Translation: اپ کیا کیا UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Input: I am learning Urdu.\n",
      "Predicted Translation: میں ایک پہ بول UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "Input: Goodbye, take care.\n",
      "Predicted Translation: الوداع مہاراج nan nan UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Input: Hello, how can I help you?\n",
      "Predicted Translation: UNK UNK UNK اپ اپ کیجئے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Input: I am from Pakistan.\n",
      "Predicted Translation: میں میں ایک نام UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Input: What is your name?\n",
      "Predicted Translation: تمہارا کا کیا ہے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "model = load_model('english_to_urdu_rnn_model.keras')\n",
    "df = pd.read_excel('English_Urdu.xlsx')\n",
    "\n",
    "# Load data\n",
    "df = df.astype(str)\n",
    "\n",
    "# Create tokenizers\n",
    "english_tokenizer = Tokenizer()\n",
    "urdu_tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizers to data\n",
    "english_tokenizer.fit_on_texts(df['English'])\n",
    "urdu_tokenizer.fit_on_texts(df['Urdu'])\n",
    "\n",
    "# Get vocabulary\n",
    "english_word_index = english_tokenizer.word_index\n",
    "urdu_word_index = urdu_tokenizer.word_index\n",
    "urdu_index_word = {v: k for k, v in urdu_word_index.items()}\n",
    "\n",
    "# Define max sequence length\n",
    "max_english_length = 40\n",
    "max_urdu_length = 40\n",
    "\n",
    "def translate(sentence):\n",
    "    sequence = english_tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_english_length, padding='post')\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    translation = []\n",
    "    for word_prob in prediction[0]:\n",
    "        predicted_index = np.argmax(word_prob)\n",
    "        word = urdu_index_word.get(predicted_index, 'UNK')\n",
    "        translation.append(word)\n",
    "    return ' '.join(translation)\n",
    "\n",
    "# Calculate BLEU score\n",
    "smooth = SmoothingFunction()\n",
    "\n",
    "# Test sentences with corresponding reference translations\n",
    "test_data = df[['English', 'Urdu']].values\n",
    "\n",
    "# Define the number of sentences to process\n",
    "num_sentences = 10\n",
    "\n",
    "# Calculate BLEU score for each sentence\n",
    "bleu_scores = []\n",
    "for i, (english_sentence, urdu_reference) in enumerate(test_data):\n",
    "    if i >= num_sentences:\n",
    "        break\n",
    "\n",
    "    predicted_translation = translate(english_sentence)\n",
    "\n",
    "    predicted_translation_tokens = word_tokenize(predicted_translation)\n",
    "    reference_translation_tokens = word_tokenize(urdu_reference)\n",
    "\n",
    "    bleu_score = sentence_bleu([reference_translation_tokens], predicted_translation_tokens, smoothing_function=smooth.method4)\n",
    "\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "    print(f\"Input: {english_sentence}\")\n",
    "    print(f\"Predicted Translation: {predicted_translation}\")\n",
    "    print(f\"Reference Translation: {urdu_reference}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Calculate overall BLEU score\n",
    "overall_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"Overall BLEU Score: {overall_bleu_score:.4f}\")\n",
    "\n",
    "# Print BLEU scores for each sentence\n",
    "print(\"BLEU Scores for each sentence:\")\n",
    "for i, bleu_score in enumerate(bleu_scores):\n",
    "    print(f\"Sentence {i+1}: {bleu_score:.4f}\")\n",
    "\n",
    "# Additional test sentences\n",
    "additional_test_sentences = [\n",
    "    \"How are you today?\",\n",
    "    \"What is your favorite food?\",\n",
    "    \"I love reading books.\",\n",
    "    \"Where do you live?\",\n",
    "    \"What do you do?\",\n",
    "    \"I am learning Urdu.\",\n",
    "    \"Goodbye, take care.\",\n",
    "    \"Hello, how can I help you?\",\n",
    "    \"I am from Pakistan.\",\n",
    "    \"What is your name?\"\n",
    "]\n",
    "\n",
    "print(\"\\nAdditional Test Sentences:\")\n",
    "for sentence in additional_test_sentences:\n",
    "    predicted_translation = translate(sentence)\n",
    "    print(f\"Input: {sentence}\")\n",
    "    print(f\"Predicted Translation: {predicted_translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52708252-b670-421e-a6ca-1217de4b3866",
   "metadata": {},
   "source": [
    "LSTM Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333595c9-04a8-4a98-a7ff-6f7ede6e6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# Download and load the parallel-corpus.xlsx dataset\n",
    "df = pd.read_excel('English_Urdu.xlsx')\n",
    "df = df.astype(str)\n",
    "\n",
    "# Preprocess data\n",
    "english_texts = df['English'].tolist()\n",
    "urdu_texts = df['Urdu'].tolist()\n",
    "\n",
    "english_tokenizer = Tokenizer()\n",
    "urdu_tokenizer = Tokenizer()\n",
    "\n",
    "english_tokenizer.fit_on_texts(english_texts)\n",
    "urdu_tokenizer.fit_on_texts(urdu_texts)\n",
    "\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "urdu_vocab_size = len(urdu_tokenizer.word_index) + 1\n",
    "\n",
    "max_length = 40\n",
    "\n",
    "english_sequences = english_tokenizer.texts_to_sequences(english_texts)\n",
    "urdu_sequences = urdu_tokenizer.texts_to_sequences(urdu_texts)\n",
    "\n",
    "english_padded = pad_sequences(english_sequences, maxlen=max_length, padding='post')\n",
    "urdu_padded = pad_sequences(urdu_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "train_size = int(0.8 * len(english_padded))\n",
    "val_size = int(0.1 * len(english_padded))\n",
    "test_size = len(english_padded) - train_size - val_size\n",
    "\n",
    "train_english, val_english, test_english = english_padded[:train_size], english_padded[train_size:train_size+val_size], english_padded[train_size+val_size:]\n",
    "train_urdu, val_urdu, test_urdu = urdu_padded[:train_size], urdu_padded[train_size:train_size+val_size], urdu_padded[train_size+val_size:]\n",
    "\n",
    "# Create dataset and data loader\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_english, train_urdu))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_english, val_urdu))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_english, test_urdu))\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.shuffle(100).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Masking, TimeDistributed, Bidirectional\n",
    "\n",
    "# Define LSTM model\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim=english_vocab_size, output_dim=256, input_length=max_length),\n",
    "    Masking(mask_value=0),\n",
    "    Bidirectional(LSTM(units=512, return_sequences=True)),\n",
    "    TimeDistributed(Dense(urdu_vocab_size, activation='softmax'))\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', \n",
    "                    loss='sparse_categorical_crossentropy', \n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM model\n",
    "history = model_lstm.fit(train_dataset, epochs=50, validation_data=val_dataset)\n",
    "# Save LSTM model\n",
    "model_lstm.save('lstm_translator.h5')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "smoothing_func = SmoothingFunction()\n",
    "\n",
    "def calculate_bleu(reference, prediction):\n",
    "    reference = word_tokenize(reference)\n",
    "    prediction = word_tokenize(prediction)\n",
    "    return sentence_bleu([reference], prediction, smoothing_function=smoothing_func.method4)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "max_test_size = 20 # Limit test size\n",
    "bleu_scores_lstm = []\n",
    "for i in range(min(max_test_size, len(test_english))):\n",
    "    reference = urdu_tokenizer.sequences_to_texts([test_urdu[i]])[0]\n",
    "    prediction = model_lstm.predict(test_english[i:i+1])\n",
    "    predicted_sequence = np.argmax(prediction, axis=2)[0]\n",
    "    prediction = urdu_tokenizer.sequences_to_texts([predicted_sequence])[0]\n",
    "    bleu_scores_lstm.append(calculate_bleu(reference, prediction))\n",
    "\n",
    "print(f'Test Average BLEU Score (LSTM): {sum(bleu_scores_lstm) / len(bleu_scores_lstm):.4f}')\n",
    "\n",
    "\n",
    "# Evaluate LSTM model on original corpus (10 sentences)\n",
    "print(\"Evaluation on Original Corpus\")\n",
    "evaluation_size = 10\n",
    "corpus_english = english_texts[:evaluation_size]\n",
    "corpus_urdu = urdu_texts[:evaluation_size]\n",
    "\n",
    "corpus_english_sequences = english_tokenizer.texts_to_sequences(corpus_english)\n",
    "corpus_urdu_sequences = urdu_tokenizer.texts_to_sequences(corpus_urdu)\n",
    "\n",
    "corpus_english_padded = pad_sequences(corpus_english_sequences, maxlen=max_length, padding='post')\n",
    "corpus_urdu_padded = pad_sequences(corpus_urdu_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "predictions_lstm = model_lstm.predict(corpus_english_padded)\n",
    "\n",
    "for i in range(min(evaluation_size, len(corpus_english))):\n",
    "    reference = corpus_urdu[i]\n",
    "    predicted_sequence = np.argmax(predictions_lstm[i], axis=1)\n",
    "    prediction = urdu_tokenizer.sequences_to_texts([predicted_sequence])[0]\n",
    "    print(f'English: {corpus_english[i]}')\n",
    "    print(f'Reference Urdu: {reference}')\n",
    "    print(f'Predicted Urdu (LSTM): {prediction}')\n",
    "    print(f'BLEU Score: {calculate_bleu(reference, prediction):.4f}')\n",
    "    print('---')\n",
    "\n",
    "# Add a termination point\n",
    "print(\"Evaluation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d759e85-d181-468b-9a88-10b4201fe714",
   "metadata": {},
   "source": [
    "LSTM Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d32224-8c26-4369-9879-c9e500a88144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987ms/step\n",
      "Input: how can i communicate with my parents\n",
      "Predicted Translation: میں اپنے والدین سے کیسے بات کروں UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: میں اپنے والدین سے کیسے بات کروں\n",
      "BLEU Score: 0.1399\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Input: how can i make friends \n",
      "Predicted Translation: میں میں کیسے بنائوں UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: میں دوست کیسے بنائوں\n",
      "BLEU Score: 0.0175\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "Input: why do i get so sad \n",
      "Predicted Translation: کیوں اتنا اداس کیوں ہے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: میں اتنا اداس کیوں ہوں\n",
      "BLEU Score: 0.0317\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "Input: if you ve asked yourself such questions you re not alone\n",
      "Predicted Translation: اگر اپ نے اپنے اپ سے ایسے سوالات کیے ہیں تو تو اکیلے کیا UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: اگر اپ نے اپنے اپ سے ایسے سوالات کیے ہیں تو اپ اکیلے نہیں ہیں\n",
      "BLEU Score: 0.2505\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Input: depending on where you ve turned for guidance you may have been given conflicting answers\n",
      "Predicted Translation: اس بات پر منحصر ہے کہ اپ رہنمائی کے کے کہاں گئے ہیں ہو سکتا گئے اپ کو جوابات جوابات دیے گئے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: اس بات پر منحصر ہے کہ اپ رہنمائی کے لیے کہاں گئے ہیں ہو سکتا ہے اپ کو متضاد جوابات دیے گئے ہوں\n",
      "BLEU Score: 0.3270\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "Input: to help young people get solid advice they can rely on awake magazine launched the biblebased series entitled  young people ask \n",
      "Predicted Translation: نوجوانوں کی ٹھوس مشورے حاصل کرنے میں مدد کرنے کے لیے جس پر وہ بھروسہ کر سکتے ہیں جاگو میگزین نے پر پر رسالہ رسالہ UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: نوجوانوں کو ٹھوس مشورے حاصل کرنے میں مدد کرنے کے لیے جس پر وہ بھروسہ کر سکتے ہیں جاگو میگزین نے بائبل پر مبنی رسالہ شروع کیا\n",
      "BLEU Score: 0.4707\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "Input: in january decades later the series still draws an enthusiastic response\n",
      "Predicted Translation: ۸ جنوری ۱۹۸۲ دہائیوں کے بعد سیریز اب بھی ایک پرجوش ردعمل کھینچتی ہے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: ۸ جنوری ۱۹۸۲ دہائیوں کے بعد سیریز اب بھی ایک پرجوش ردعمل کھینچتی ہے\n",
      "BLEU Score: 0.3235\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "Input: each article is the product of extensive research in fact to determine just how young people think and feel awake\n",
      "Predicted Translation: درحقیقت اس بات کا تعین کرنے کے لیے کہ نوجوان کہ سوچتے اور محسوس کرتے ہیں جاگو UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: درحقیقت اس بات کا تعین کرنے کے لیے کہ نوجوان کیسے سوچتے اور محسوس کرتے ہیں جاگو\n",
      "BLEU Score: 0.3327\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "Input: the book you now hold was originally published in \n",
      "Predicted Translation: کیا کتاب اپ کے پاس اس وہ اصل میں ۱۹۸۹ میں ہوئی ہوئی تھی UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: جو کتاب اپ کے پاس ہے وہ اصل میں ۱۹۸۹ میں شائع ہوئی تھی\n",
      "BLEU Score: 0.1566\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Input: however the chapters have been completely revised to address the issues of today\n",
      "Predicted Translation: اج کے مسائل کو حل کرنے کے لیے ابواب پر مکمل نظر ثانی کی ہے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "Reference Translation: اج کے مسائل کو حل کرنے کے لیے ابواب پر مکمل نظر ثانی کی گئی ہے\n",
      "BLEU Score: 0.3291\n",
      "\n",
      "Overall BLEU Score: 0.2379\n",
      "BLEU Scores for each sentence:\n",
      "Sentence 1: 0.1399\n",
      "Sentence 2: 0.0175\n",
      "Sentence 3: 0.0317\n",
      "Sentence 4: 0.2505\n",
      "Sentence 5: 0.3270\n",
      "Sentence 6: 0.4707\n",
      "Sentence 7: 0.3235\n",
      "Sentence 8: 0.3327\n",
      "Sentence 9: 0.1566\n",
      "Sentence 10: 0.3291\n",
      "\n",
      "Additional Test Sentences:\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "Input: How are you today?\n",
      "Predicted Translation: اپ اج کر ہو ہیں ہیں UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "Input: What is your favorite food?\n",
      "Predicted Translation: اپ کیا کا کے ہے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "Input: I love reading books.\n",
      "Predicted Translation: مجھے کہاں پسند کتابیں کرتا ہے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "Input: Where do you live?\n",
      "Predicted Translation: اپ کہاں رہتے ہیں UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Input: What do you do?\n",
      "Predicted Translation: اپ کیا کیا UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Input: I am learning Urdu.\n",
      "Predicted Translation: میں بہت جرمن سیکھ ہوں UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Input: Goodbye, take care.\n",
      "Predicted Translation: بہت اپنا قابل رکھنا UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "Input: Hello, how can I help you?\n",
      "Predicted Translation: nan UNK کیا میں سکتا کی کی UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Input: I am from Pakistan.\n",
      "Predicted Translation: میں میں سے ہے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Input: What is your name?\n",
      "Predicted Translation: اپ تمہارا کیا ہے UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel('English_Urdu.xlsx')\n",
    "df = df.astype(str)\n",
    "\n",
    "\n",
    "# Create tokenizers\n",
    "english_tokenizer = Tokenizer()\n",
    "urdu_tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "# Fit tokenizers to data\n",
    "english_tokenizer.fit_on_texts(df['English'])\n",
    "urdu_tokenizer.fit_on_texts(df['Urdu'])\n",
    "\n",
    "\n",
    "# Get vocabulary\n",
    "english_word_index = english_tokenizer.word_index\n",
    "urdu_word_index = urdu_tokenizer.word_index\n",
    "urdu_index_word = {v: k for k, v in urdu_word_index.items()}\n",
    "\n",
    "\n",
    "# Define max sequence length\n",
    "max_english_length = 40\n",
    "max_urdu_length = 40\n",
    "\n",
    "\n",
    "# Load pre-trained LSTM model\n",
    "model = load_model('lstm_original.h5')\n",
    "\n",
    "\n",
    "# Define translation function\n",
    "def translate(sentence):\n",
    "    sequence = english_tokenizer.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_english_length, padding='post')\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    translation = []\n",
    "    for word_prob in prediction[0]:\n",
    "        predicted_index = np.argmax(word_prob)\n",
    "        word = urdu_index_word.get(predicted_index, 'UNK')\n",
    "        translation.append(word)\n",
    "    return ' '.join(translation)\n",
    "\n",
    "\n",
    "# Calculate BLEU score\n",
    "smooth = SmoothingFunction()\n",
    "\n",
    "\n",
    "# Test sentences with corresponding reference translations\n",
    "test_data = df[['English', 'Urdu']].values\n",
    "\n",
    "\n",
    "# Define the number of sentences to process\n",
    "num_sentences = 10\n",
    "\n",
    "\n",
    "# Calculate BLEU score for each sentence\n",
    "bleu_scores = []\n",
    "for i, (english_sentence, urdu_reference) in enumerate(test_data):\n",
    "    if i >= num_sentences:\n",
    "        break\n",
    "\n",
    "    predicted_translation = translate(english_sentence)\n",
    "\n",
    "    predicted_translation_tokens = word_tokenize(predicted_translation)\n",
    "    reference_translation_tokens = word_tokenize(urdu_reference)\n",
    "\n",
    "    bleu_score = sentence_bleu([reference_translation_tokens], predicted_translation_tokens, smoothing_function=smooth.method4)\n",
    "\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "    print(f\"Input: {english_sentence}\")\n",
    "    print(f\"Predicted Translation: {predicted_translation}\")\n",
    "    print(f\"Reference Translation: {urdu_reference}\")\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Calculate overall BLEU score\n",
    "overall_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"Overall BLEU Score: {overall_bleu_score:.4f}\")\n",
    "\n",
    "\n",
    "# Print BLEU scores for each sentence\n",
    "print(\"BLEU Scores for each sentence:\")\n",
    "for i, bleu_score in enumerate(bleu_scores):\n",
    "    print(f\"Sentence {i+1}: {bleu_score:.4f}\")\n",
    "\n",
    "\n",
    "# Additional test sentences\n",
    "additional_test_sentences = [\n",
    "    \"How are you today?\",\n",
    "    \"What is your favorite food?\",\n",
    "    \"I love reading books.\",\n",
    "    \"Where do you live?\",\n",
    "    \"What do you do?\",\n",
    "    \"I am learning Urdu.\",\n",
    "    \"Goodbye, take care.\",\n",
    "    \"Hello, how can I help you?\",\n",
    "    \"I am from Pakistan.\",\n",
    "    \"What is your name?\"\n",
    "]\n",
    "\n",
    "print(\"\\nAdditional Test Sentences:\")\n",
    "for sentence in additional_test_sentences:\n",
    "    predicted_translation = translate(sentence)\n",
    "    print(f\"Input: {sentence}\")\n",
    "    print(f\"Predicted Translation: {predicted_translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc7bbc-c8e8-4a19-9ca1-e2d8d9a63742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
