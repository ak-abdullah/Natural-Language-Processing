<h1 align="center">ğŸ§  Natural Language Processing Projects</h1>


  A compilation of practical and theoretical NLP work involving text classification, sequence modeling, machine translation, and transformer-based architectures.


---

## ğŸ“Œ Project Overview

This repository highlights a series of NLP projects that cover the full pipeline â€” from data preprocessing and embedding generation to model training and fine-tuning on real-world tasks.

## ğŸš€ What's Inside

### 1. ğŸ§¹ NLP Preprocessing & Embeddings
- Performed end-to-end **text preprocessing**:
  - Tokenization
  - Lowercasing
  - Stopword removal
  - Lemmatization
  - Noise cleaning (punctuation, numbers, special characters)
- Generated **word embeddings** using techniques like Word2Vec and GloVe for downstream tasks.

### 2. ğŸ—£ï¸ Sentiment Classification using Naive Bayes
- Implemented a **Multinomial Naive Bayes** classifier to classify user reviews as:
  - **Positive**
  - **Neutral**
  - **Negative**
- Used cleaned data and TF-IDF vectorization for feature representation.
- Evaluated using accuracy, precision, recall, and confusion matrix.

### 3. ğŸ”„ English â†’ Urdu Translation with RNN & LSTM
- Built sequence-to-sequence models using:
  - Vanilla RNNs
  - Long Short-Term Memory networks (LSTMs)
- Trained on paired Englishâ€“Urdu sentence datasets.
- Integrated attention mechanism for improved translation performance.

### 4. ğŸ”§ Fine-Tuning LLMs (Phi-3)
- Fine-tuned the **Phi-3** small language model on custom NLP tasks.
- Applied prompt engineering and task-specific finetuning techniques.
- Evaluated performance on few-shot and zero-shot tasks.

### 5. ğŸ§  In-depth Study of NLP Architectures
- Explored and documented key modern NLP models:
  - **Transformer architecture** and self-attention mechanism.
  - **BERT (Bidirectional Encoder Representations from Transformers)** â€” embeddings, masked language modeling, fine-tuning.
- Compared performance and trade-offs across models.

---


